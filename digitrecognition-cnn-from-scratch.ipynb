{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# a website that explains the back-prop for CNN clearly: https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c\nimport numpy as np\nimport pandas as pd\nimport math\nimport os\nimport sys\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom sklearn.model_selection import train_test_split\n# np.set_printoptions(threshold=sys.maxsize)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\ncurrent_dir = %pwd\ntrain_data = pd.read_csv('../input/digit-recognizer/train.csv',dtype = np.float32)\ntest_data = pd.read_csv('../input/digit-recognizer/test.csv',dtype = np.float32)\n\n# split data into features/labels\ny_train_data = train_data.label.values\nx_train_data = train_data.loc[:,train_data.columns != \"label\"].values/255 # normalization\nx_train_data=x_train_data.reshape(x_train_data.shape[0],28,28)\nx_test = test_data.values/255\nx_test = test_data.to_numpy().reshape(x_test.shape[0],28,28)\nx_train_data = np.expand_dims(x_train_data, axis=3)\nx_test = np.expand_dims(x_test, axis=3)\n\n# split train/dev data\nx_train, x_dev, y_train, y_dev = train_test_split(x_train_data, y_train_data, test_size=0.33, random_state=42)\nprint(len(x_train))\nprint(len(x_dev))\nprint(len(x_test))\nprint(x_train.shape)\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Utils\ndef relu(x):\n    return np.maximum(x,0)\n\ndef soft_max(x): # x dim: [no_samples,output_classes]\n    exponentials = np.exp(x)\n    sum_exponentials = exponentials.sum(axis=1).reshape(exponentials.shape[0],-1)\n    result = exponentials/sum_exponentials\n    return result\ndef cross_entropy_loss(predicted,answer):\n    return -y*np.log(predicted+1e-6).sum(axis=1)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Conv2d: # w's in the transition between layers\n    def __init__(self, channels_in, channels_out, kernel_size, stride=1, padding=0):\n        self.channels_in=channels_in\n        self.channels_out=channels_out\n        self.kernel_size=kernel_size\n        self.stride=stride\n        self.padding=padding\n        self.weights = None\n        self.in_data = None\n        self.out_data = None\n        \n    def __call__(self,x):\n        # if it has a batch_size of 1 for x_test or just plain sgd\n        if len(x.shape) == 3:\n            np.expand_dims(x, axis=0)\n            \n        # if weights haven't been initilized, then initilize them using xavier's method\n        if self.weights is None: # weights dim: [co, kernel_size, kernel_size, ci]\n            self.weights = self.xavier_initialization(x)\n        # padding\n        if self.padding:\n            x = np.pad(x, ((0, 0), (self.padding, self.padding), (self.padding, self.padding), (0, 0)),'constant')\n        \n        # Convolving\n        y = np.zeros((self.channels_out,\n                     np.floor((x.shape[1]-self.kernel_size+1)/self.stride).astype(int),\n                     np.floor((x.shape[2]-self.kernel_size+1)/self.stride).astype(int),\n                     x.shape[0])) # y dimensions: [co][start_h][start_w][batch_size] (will swap 0&3 dimensions later)\n        for co in range (self.channels_out):\n            for start_h in range(0,x.shape[1]-self.kernel_size+1,self.stride): # height\n                for start_w in range(0,x.shape[2]-self.kernel_size+1,self.stride):\n                    # example dimension for the x below this line: 64*5*5*100(64 batch_size, 100 channels_in)\n                    covariance = np.multiply(x[:, start_h:start_h+self.kernel_size, start_w:start_w+self.kernel_size, :],\n                    self.weights[co]) # example dimension: 5*5*100(100 channels_in)\n\n                    \n                    y[co, np.floor(start_h/self.stride).astype(int), np.floor(start_w/self.stride).astype(int)] = \\\n                    np.array(np.sum(covariance,axis=(1,2,3)).reshape(2))\n        y = np.swapaxes(y, 0, 3) \n        self.in_data = x\n        self.out_data = y\n        return self.out_data # [batch_size][start_h][start_w][channels_out]\n    \n    def xavier_initialization(self, x):\n        fan_in = x.shape[1]*x.shape[2]*x.shape[3] #(batch size too? then add x.shape[0] too)\n        fan_out = self.channels_out*np.floor((x.shape[1]-self.kernel_size+1)/self.stride).astype(int)*np.floor((x.shape[2]-self.kernel_size+1)/self.stride).astype(int)\n        \n        return np.random.randn(self.channels_out, self.kernel_size, self.kernel_size, self.channels_in)*np.sqrt(2/(fan_in+fan_out))\n#         return np.arange(out1*ks*ks*in1).reshape(out1,ks,ks,in1) #random\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing Conv2d\nxr = np.random.randn(2*8*8*1).reshape(2,8,8,1)\nc = Conv2d(1, 10, 3)\nyr = c(xr)\n# print(yr)\nprint(yr[1,:,:,0])\nyr.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MaxPool2d: # w's in the transition between layers\n    def __init__(self, kernel_size, stride, padding=0):\n        self.kernel_size=kernel_size\n        self.stride=stride\n        self.padding=padding\n        self.in_data = None\n        self.out_data = None\n        \n    def __call__(self,x):\n        # if it has a batch_size of 1 for x_test or just plain sgd\n        if len(x.shape) == 3:\n            np.expand_dims(x, axis=0)\n                                                                ###\n        # padding\n        if self.padding:\n            x = np.pad(x, ((0, 0), (self.padding, self.padding), (self.padding, self.padding), (0, 0)),'constant')\n        \n        # MaxPooling\n        x = np.swapaxes(x, 1, 3) # new x dim: [batch_size][co][start_w][start_h] will swap 1&3 back later\n        y = np.zeros((x.shape[0],\n                     x.shape[1],\n                     np.floor((x.shape[2])/self.stride).astype(int),\n                     np.floor((x.shape[3])/self.stride).astype(int))\n                     )# y dimensions: [batch_size][co][start_h][start_w] (will swap 1&3 dimensions later)\n        for start_h in range(0,x.shape[3],self.stride): # height\n            for start_w in range(0,x.shape[2],self.stride):\n                y[:, :, np.floor(start_w/self.stride).astype(int), np.floor(start_h/self.stride).astype(int)] = \\\n                np.amax(x[:, :,start_w:start_w+self.kernel_size,start_h:start_h+self.kernel_size], axis=(2,3))\n                \n        y = np.swapaxes(y, 1, 3) \n        x = np.swapaxes(x, 1, 3) \n        self.in_data = x\n        self.out_data = y\n        return self.out_data # [batch_size][start_h][start_w][channels_out]\ndef relu(x):\n    return np.maximum(x,0)\ndef soft_max(x): # x dim: [no_samples,output_classes]\n    exponentials = np.exp(x)\n    sum_exponentials = exponentials.sum(axis=1).reshape(exponentials.shape[0],-1)\n    result = exponentials/sum_exponentials\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing MaxPool & relu\nre_yr = relu(yr)\nprint(re_yr[1,1,4,0])\nprint(re_yr[1,1,5,0])\nprint(re_yr[1,2,4,0])\nprint(re_yr[1,2,5,0])\nprint(re_yr[1,0,3,0])\nprint(re_yr[1,1,3,0])\nprint(re_yr[1,0,4,0])\nprint(re_yr[1,0,5,0])\nprint(re_yr[1,2,3,0])\n# print(re_yr.shape)\nm = MaxPool2d(3,3)\nzr = m(re_yr)\nprint(f\"max of those 9 elements is: {zr[1,0,1,0]}\")\nprint(zr.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Linear: # w's in the transition between layers\n    def __init__(self, in_size, out_size):\n        self.in_size=in_size\n        self.out_size=out_size\n        self.weights = None\n        self.in_data = None\n        self.out_data = None\n        \n    def __call__(self,x):\n        if self.weights is None: # weights dim: [co, kernel_size, kernel_size, ci]\n            self.weights = np.random.randn(self.in_size,self.out_size)/100\n        y = x@self.weights\n        self.in_data = x\n        self.out_data = y\n        return y\n    def backward(self):\n        \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing Linear\nx= np.random.rand(3,2,5,10)\nx = x.reshape(-1, 100)\nprint(x.shape)\nl = Linear(100,10)\npr = np.arange(2000).reshape(20,100)\nqr = l(pr)\nprint(qr.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNN:\n    def __init__(self):\n        self.conv1 = Conv2d(1, 64, 5) # 28*28 ->  24*24\n        self.pool1 = MaxPool2d(3,stride=3) # 24*24 ->8*8\n        self.fc1 = nn.Linear(64*8*8, 10)\n        \n    def Forward(self, x):\n        x = self.pool(relu(self.conv1(x)) )\n        x = x.reshape(-1, 64 * 8 * 8)\n        x = self.fc1(x)\n        exponentials = np.exp(x)\n        sum_exponentials = np.sum(e, axis=1)\n        result = exponentials/sum_exponentials\n        return x\n        \n    def Backward(self):\n        self.fc1.backward()\n        \n\t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn = CNN()\ncnn.Forward()\ncnn.Backward()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}